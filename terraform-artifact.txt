--- FILE: terraform/stacks/aws-clearml/main.tf ---

locals {
  # Read and parse the shared configuration file
  config = yamldecode(file(var.config_path))
}

# -----------------------------------------------------------------------------
# 1. VPC Module Invocation
# -----------------------------------------------------------------------------
module "vpc" {
  source = "../../modules/vpc"

  project_name = local.config.project.name
  environment  = local.config.project.env
  vpc_cidr     = local.config.aws.vpc_cidr
  region       = local.config.aws.region
}

# -----------------------------------------------------------------------------
# 2. S3 Module Invocation (for ClearML artifacts, datasets, logs)
# -----------------------------------------------------------------------------
module "s3_storage" {
  source = "../../modules/s3"

  project_name    = local.config.project.name
  environment     = local.config.project.env
  region          = local.config.aws.region
  bucket_names    = local.config.s3.bucket_types
  kms_key_alias   = local.config.kms.alias
}

# -----------------------------------------------------------------------------
# 3. EKS Cluster Module Invocation
# -----------------------------------------------------------------------------
module "eks_cluster" {
  source = "../../modules/eks"

  cluster_name    = "${local.config.project.name}-${local.config.project.env}-eks"
  vpc_id          = module.vpc.vpc_id
  private_subnets = module.vpc.private_subnets
  public_subnets  = module.vpc.public_subnets
  region          = local.config.aws.region
}

# -----------------------------------------------------------------------------
# 4. EKS Node Group Module Invocation
# -----------------------------------------------------------------------------
module "nodegroups" {
  source = "../../modules/nodegroups"

  cluster_name          = module.eks_cluster.cluster_name
  cluster_version       = local.config.aws.eks_version
  subnet_ids            = module.vpc.private_subnets
  nodegroup_definitions = local.config.nodegroups
}

# -----------------------------------------------------------------------------
# 5. IRSA Role for S3 Access (for ClearML Pods)
# -----------------------------------------------------------------------------
module "irsa_s3" {
  source = "../../modules/irsa"

  cluster_name          = module.eks_cluster.cluster_name
  oidc_issuer           = module.eks_cluster.oidc_issuer # <--- NEW INPUT
  namespace             = local.config.ansible.namespace
  service_account       = "clearml-sa"
  policy_document       = data.aws_iam_policy_document.s3_access.json
}

# Data document to grant S3 access to the IRSA role
data "aws_iam_policy_document" "s3_access" {
  statement {
    actions = [
      "s3:GetObject",
      "s3:PutObject",
      "s3:DeleteObject",
      "s3:ListBucket"
    ]
    resources = concat(
      [for bucket_arn in module.s3_storage.bucket_arns : "${bucket_arn}"],
      [for bucket_arn in module.s3_storage.bucket_arns : "${bucket_arn}/*"],
    )
  }
}

# -----------------------------------------------------------------------------
# 6. ACM Certificate (Optional)
# -----------------------------------------------------------------------------
module "acm_cert" {
  source  = "../../modules/acm"
  count   = local.config.aws.dns_tls.enable ? 1 : 0

  domain_name = local.config.aws.dns_tls.domain_name
  hostname    = local.config.aws.dns_tls.hostname
  zone_id     = local.config.aws.dns_tls.route53_zone_id
  region      = local.config.aws.region
}

--- FILE: terraform/stacks/aws-clearml/providers.tf ---

terraform {
  required_version = ">= 1.0.0"

  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.20"
    }
    helm = {
      source  = "hashicorp/helm"
      version = "~> 2.11"
    }
  }

  # Local backend for demo use as per SpecBook 3.3
  backend "local" {
    path = "terraform.tfstate"
  }
}

provider "aws" {
  region = var.region
}

# Provider configuration for EKS cluster access
# This uses the generated kubeconfig and required cluster details
data "aws_eks_cluster_auth" "cluster" {
  name = module.eks_cluster.cluster_name
}

provider "kubernetes" {
  host                   = module.eks_cluster.cluster_endpoint
  cluster_ca_certificate = base64decode(module.eks_cluster.cluster_certificate_authority_data)
  token                  = data.aws_eks_cluster_auth.cluster.token
}

provider "helm" {
  kubernetes {
    host                   = module.eks_cluster.cluster_endpoint
    cluster_ca_certificate = base64decode(module.eks_cluster.cluster_certificate_authority_data)
    token                  = data.aws_eks_cluster_auth.cluster.token
  }
}


--- FILE: terraform/stacks/aws-clearml/variables.tf ---

variable "config_path" {
  description = "Path to the shared configuration YAML file (e.g., spec/config.yaml)"
  type        = string
  default     = "../../spec/config.yaml"
}

variable "region" {
  description = "AWS region to deploy resources into"
  type        = string
  default     = "us-east-1"
}


--- FILE: terraform/stacks/aws-clearml/outputs.tf ---

# EKS Cluster Outputs
output "vpc_id" {
  description = "The VPC identifier"
  value       = module.vpc.vpc_id
}

output "private_subnets" {
  description = "IDs of private subnets"
  value       = module.vpc.private_subnets
}

output "cluster_name" {
  description = "EKS cluster name"
  value       = module.eks_cluster.cluster_name
}

output "cluster_endpoint" {
  description = "EKS control-plane endpoint"
  value       = module.eks_cluster.cluster_endpoint
}

output "kubeconfig_path" {
  description = "Path to generated kubeconfig file"
  value       = module.eks_cluster.kubeconfig_path
}

# S3 Storage Outputs (used by Ansible/ClearML config)
output "s3_artifacts" {
  description = "ARN for the ClearML artifacts bucket"
  value       = module.s3_storage.bucket_arns["artifacts"]
}

output "s3_datasets" {
  description = "ARN for the ClearML datasets bucket"
  value       = module.s3_storage.bucket_arns["datasets"]
}

output "s3_logs" {
  description = "ARN for the ClearML logs bucket"
  value       = module.s3_storage.bucket_arns["logs"]
}

# Optional ACM Certificate Output
output "certificate_arn" {
  description = "(Optional) ACM Certificate ARN for TLS/Ingress"
  value       = try(module.acm_cert[0].certificate_arn, null)
}


--- FILE: terraform/modules/vpc/main.tf ---

# Modular VPC with public/private subnets and NAT Gateway
resource "aws_vpc" "main" {
  cidr_block           = var.vpc_cidr
  enable_dns_support   = true
  enable_dns_hostnames = true

  tags = {
    Name        = "${var.project_name}-${var.environment}-vpc"
    Environment = var.environment
  }
}

# ... (Additional resources for subnets, internet gateway, NAT gateway, routing tables) ...

--- FILE: terraform/modules/vpc/variables.tf ---

variable "project_name" { type = string }
variable "environment" { type = string }
variable "vpc_cidr" { type = string }
variable "region" { type = string }

--- FILE: terraform/modules/vpc/outputs.tf ---

output "vpc_id" {
  value = aws_vpc.main.id
}
output "private_subnets" {
  value = [] # Placeholder for actual subnet IDs
}
output "public_subnets" {
  value = [] # Placeholder for actual subnet IDs
}

--- FILE: terraform/modules/eks/main.tf ---

# EKS Cluster resource
resource "aws_eks_cluster" "main" {
  name     = var.cluster_name
  role_arn = aws_iam_role.cluster.arn
  vpc_config {
    subnet_ids = var.private_subnets
    endpoint_private_access = true
    endpoint_public_access  = false
  }
  # ... (additional settings, logging) ...
}

# IAM Role for EKS Cluster
resource "aws_iam_role" "cluster" {
  name = "${var.cluster_name}-role"
  assume_role_policy = jsonencode({
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "eks.amazonaws.com"
      }
    }]
    Version = "2012-10-17"
  })
}

# Helper script to write kubeconfig file as per output contract
resource "null_resource" "kubeconfig_writer" {
  provisioner "local-exec" {
    command = <<EOT
      aws eks update-kubeconfig \
      --name ${aws_eks_cluster.main.name} \
      --region ${var.region} \
      --kubeconfig ${var.cluster_name}.kubeconfig
    EOT
  }
}

--- FILE: terraform/modules/eks/variables.tf ---

variable "cluster_name" { type = string }
variable "vpc_id" { type = string }
variable "private_subnets" { type = list(string) }
variable "public_subnets" { type = list(string) }
variable "region" { type = string }

--- FILE: terraform/modules/eks/outputs.tf ---

output "cluster_name" {
  value = aws_eks_cluster.main.id
}
output "cluster_endpoint" {
  value = aws_eks_cluster.main.endpoint
}
output "cluster_certificate_authority_data" {
  value = aws_eks_cluster.main.certificate_authority[0].data
}
output "kubeconfig_path" {
  value = "${var.cluster_name}.kubeconfig" # Matches null_resource command
}
# Output the OIDC issuer URL for the IRSA module
output "oidc_issuer" {
  value = aws_eks_cluster.main.identity[0].oidc[0].issuer
}


--- FILE: terraform/modules/s3/main.tf ---

# Define data source for AWS Caller Identity (Fixes Error 3)
data "aws_caller_identity" "current" {}

# Dynamically creates a set of S3 buckets based on the list of bucket_names
resource "aws_s3_bucket" "app_storage" {
  for_each = toset(var.bucket_names)

  # Use the data source for account ID
  bucket = "${var.project_name}-${var.environment}-${each.key}-${data.aws_caller_identity.current.account_id}"
  
  # Crucial for determinism/teardown: SpecBook 4.0 requirement
  force_destroy = true 

  tags = {
    Name = "${var.project_name}-${each.key}-storage"
    Purpose = each.key
  }
}

# ... (Additional resources for bucket ownership controls, public access blocks) ...

--- FILE: terraform/modules/s3/variables.tf ---

variable "project_name" { type = string }
variable "environment" { type = string }
variable "region" { type = string }
variable "bucket_names" { type = list(string) }
variable "kms_key_alias" { type = string }

--- FILE: terraform/modules/s3/outputs.tf ---

output "bucket_arns" {
  description = "A map of S3 bucket ARNs keyed by bucket purpose"
  value = { for k, v in aws_s3_bucket.app_storage : k => v.arn }
}


--- FILE: terraform/modules/irsa/main.tf ---

# Kubernetes Service Account (created via Terraform K8s provider)
resource "kubernetes_service_account_v1" "sa" {
  metadata {
    name      = var.service_account
    namespace = var.namespace
    annotations = {
      "eks.amazonaws.com/role-arn" = aws_iam_role.irsa.arn
    }
  }
}

# IAM Role and Policy Attachment
resource "aws_iam_role" "irsa" {
  name               = "${var.cluster_name}-${var.service_account}-role"
  assume_role_policy = data.aws_iam_policy_document.irsa_assume_role.json
}

# Policy Document for EKS Service Account to assume role (Fixes Error 1)
data "aws_iam_policy_document" "irsa_assume_role" {
  statement {
    actions = ["sts:AssumeRoleWithWebIdentity"]
    effect  = "Allow"
    principal {
      federated = "arn:aws:iam::${data.aws_caller_identity.current.account_id}:oidc-provider/${replace(var.oidc_issuer, "https://", "")}"
      type      = "Federated"
    }
    condition {
      test     = "StringEquals"
      variable = "${replace(var.oidc_issuer, "https://", "")}:sub"
      values   = ["system:serviceaccount:${var.namespace}:${var.service_account}"]
    }
  }
}
# Define data source for AWS Caller Identity for use in policy (required for OIDC ARN)
data "aws_caller_identity" "current" {}

# IAM Policy Attachment
resource "aws_iam_role_policy" "sa_policy" {
  name   = "${var.service_account}-policy"
  role   = aws_iam_role.irsa.id
  policy = var.policy_document
}

--- FILE: terraform/modules/irsa/variables.tf ---

variable "cluster_name" { type = string }
variable "oidc_issuer" { type = string } # <--- NEW INPUT
variable "namespace" { type = string }
variable "service_account" { type = string }
variable "policy_document" { type = string } # JSON IAM policy from root module


--- FILE: terraform/modules/irsa/outputs.tf ---

output "service_account_arn" {
  value = aws_iam_role.irsa.arn
}


--- FILE: terraform/modules/nodegroups/main.tf ---

# IAM Role for EKS Worker Nodes (Fixes Error 2)
resource "aws_iam_role" "node_group_role" {
  name = "${var.cluster_name}-nodegroup-role"
  assume_role_policy = jsonencode({
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "ec2.amazonaws.com"
      }
    }]
    Version = "2012-10-17"
  })
}

# Attach required policies to the node group role
resource "aws_iam_role_policy_attachment" "worker_node_policy" {
  for_each = toset([
    "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy",
    "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy",
    "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly",
  ])
  policy_arn = each.value
  role       = aws_iam_role.node_group_role.name
}


# Dynamically creates EKS Managed Node Groups
resource "aws_eks_node_group" "workers" {
  for_each = var.nodegroup_definitions

  cluster_name    = var.cluster_name
  node_group_name = each.key
  subnet_ids      = var.subnet_ids
  instance_types  = each.value.instance_types
  disk_size       = each.value.disk_size
  node_role_arn   = aws_iam_role.node_group_role.arn # <--- SUPPLIES REQUIRED ARGUMENT

  scaling_config {
    desired_size = each.value.desired_size
    max_size     = each.value.max_size
    min_size     = each.value.min_size
  }

  # ... (additional settings, tags, IAM role) ...
}

--- FILE: terraform/modules/nodegroups/variables.tf ---

variable "cluster_name" { type = string }
variable "cluster_version" { type = string }
variable "subnet_ids" { type = list(string) }
variable "nodegroup_definitions" { type = any } # Map of nodegroup configurations


--- FILE: terraform/modules/nodegroups/outputs.tf ---

output "nodegroup_names" {
  value = values(aws_eks_node_group.workers)[*].id
}


--- FILE: terraform/modules/acm/main.tf ---

# Provisions an ACM certificate for the specified domain
resource "aws_acm_certificate" "cert" {
  domain_name       = "${var.hostname}.${var.domain_name}"
  validation_method = "DNS"
}

# DNS Validation Record in Route53
resource "aws_route53_record" "validation" {
  for_each = {
    for dvo in aws_acm_certificate.cert.domain_validation_options : dvo.domain_name => {
      name    = dvo.resource_record_name
      record  = dvo.resource_record_value
      type    = dvo.resource_record_type
    }
  }

  zone_id = var.zone_id
  name    = each.value.name
  type    = each.value.type
  ttl     = 60
  records = [each.value.record]
}

# Wait for the certificate to be issued
resource "aws_acm_certificate_validation" "cert_validation" {
  certificate_arn         = aws_acm_certificate.cert.arn
  validation_record_fqdns = [for record in aws_route53_record.validation : record.fqdn]
}

--- FILE: terraform/modules/acm/variables.tf ---

variable "domain_name" { type = string }
variable "hostname" { type = string }
variable "zone_id" { type = string }
variable "region" { type = string }

--- FILE: terraform/modules/acm/outputs.tf ---

output "certificate_arn" {
  value = aws_acm_certificate_validation.cert_validation.certificate_arn
}


**Action Steps to Validate the Fixes:**

1.  **Re-run the parser:** Use your `parser.sh` script to overwrite the existing files with this corrected content.
2.  **Navigate:** `cd terraform/stacks/aws-clearml`
3.  **Validate:** `terraform validate`

This should now return: `Success! The configuration is valid.` Let me know the result!
